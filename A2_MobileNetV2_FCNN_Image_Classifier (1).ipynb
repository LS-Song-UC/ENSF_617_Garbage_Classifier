{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58dde80f",
      "metadata": {
        "id": "58dde80f"
      },
      "source": [
        "Image CNN with transfer learning from MobileNETV2 and FCNN head for classification:\n",
        "\n",
        "Structure/Workflow:\n",
        "\n",
        "Image ->\n",
        "MobileNetV2 (pretrained) ->\n",
        "Global pooled features (1280-d) ->\n",
        "Projection layer (optional, recommended) ->\n",
        "Classifier head (FCNN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gT1MQoZZ2lt3",
      "metadata": {
        "id": "gT1MQoZZ2lt3"
      },
      "source": [
        "Expected Folder Strucure(expand to read):\n",
        "\n",
        "dataset_root/\n",
        "├── train/\n",
        "│   ├── Blue/\n",
        "│   │   ├── image_001.jpg\n",
        "│   │   ├── image_002.jpg\n",
        "│   │   └── ...\n",
        "│   ├── Green/\n",
        "│   │   └── ...\n",
        "│   ├── Black/\n",
        "│   │   └── ...\n",
        "│   └── Other/\n",
        "│       └── ...\n",
        "├── val/\n",
        "│   ├── Blue/\n",
        "│   ├── Green/\n",
        "│   ├── Black/\n",
        "│   └── Other/\n",
        "└── test/\n",
        "    ├── Blue/\n",
        "    ├── Green/\n",
        "    ├── Black/\n",
        "    └── Other/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42831d3f",
      "metadata": {
        "id": "42831d3f"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e138a0a0",
      "metadata": {
        "id": "e138a0a0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157de3a8",
      "metadata": {
        "id": "157de3a8"
      },
      "source": [
        "Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4d31d911",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d31d911",
        "outputId": "d35e7ae1-23f6-4bd7-b3a9-d0b84f0324bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3b9a0b",
      "metadata": {
        "id": "0f3b9a0b"
      },
      "source": [
        "Paths/Parameters/Settings definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324a8f9d",
      "metadata": {
        "id": "324a8f9d"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "NUM_CLASSES = 4\n",
        "EPOCHS = 30\n",
        "UNFREEZE_EPOCH = 0.8*EPOCHS\n",
        "INITIAL_LR = 1e-3\n",
        "FINE_TUNE_LR = 1e-5\n",
        "SAVE_MODEL_PATH = \"best_image_model.pth\"\n",
        "MISCLASS_FILE = \"misclassified.txt\"\n",
        "LOSS_PLOT_PATH = \"loss_curve.png\"\n",
        "TRAIN_PATH = \"dataset/train\"\n",
        "VAL_PATH = \"dataset/val\"\n",
        "TEST_PATH = \"dataset/test\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acc8239",
      "metadata": {
        "id": "5acc8239"
      },
      "source": [
        "Data Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa898112",
      "metadata": {
        "id": "aa898112"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2), #Optional, will monitor effects.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])])  #Stats need to align with that of MobileNetV2, no stat calculations needed on input data.\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  #Testing data MUST NOT be pre-processed. No rotations/flips/jitter applied.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726d222e",
      "metadata": {
        "id": "726d222e"
      },
      "source": [
        " Datasets and Data-Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea49382",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ea49382",
        "outputId": "3a66d619-c56f-4c95-8152-5a252976dadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 200\n",
            "Val samples: 205\n",
            "Test samples: 161\n"
          ]
        }
      ],
      "source": [
        "import shutil # Added for removing directories\n",
        "\n",
        "def remove_ipynb_checkpoints(root_dir):\n",
        "    if not os.path.exists(root_dir):\n",
        "        print(f\"Warning: Root directory {root_dir} does not exist. Skipping cleanup.\")\n",
        "        return\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        if '.ipynb_checkpoints' in dirnames:\n",
        "            checkpoints_path = os.path.join(dirpath, '.ipynb_checkpoints')\n",
        "            print(f\"Removing .ipynb_checkpoints directory: {checkpoints_path}\")\n",
        "            shutil.rmtree(checkpoints_path)\n",
        "            # Remove from dirnames so os.walk doesn't try to enter it after deletion\n",
        "            dirnames.remove('.ipynb_checkpoints')\n",
        "\n",
        "# Applied as cleanup before creating ImageFolder datasets\n",
        "remove_ipynb_checkpoints(TRAIN_PATH)\n",
        "remove_ipynb_checkpoints(VAL_PATH)\n",
        "remove_ipynb_checkpoints(TEST_PATH)\n",
        "\n",
        "train_dataset = ImageFolder(root=TRAIN_PATH, transform=train_transform)\n",
        "val_dataset = ImageFolder(root=VAL_PATH, transform=train_transform)\n",
        "test_dataset = ImageFolder(root=TEST_PATH, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "#Verifying sizes/dimension\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Val samples:\", len(val_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5459a0cc",
      "metadata": {
        "id": "5459a0cc"
      },
      "source": [
        "MobileNetV2 Image Encoder Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb23e7b",
      "metadata": {
        "id": "cfb23e7b"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, pretrained=True, freeze_backbone=True, proj_dim=256):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v2(\n",
        "            weights=models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
        "        )\n",
        "        self.backbone.classifier = nn.Identity()  # Remove default classifier, will attach own FCNN head later.\n",
        "\n",
        "        if freeze_backbone: #Initialize backbone to frozen, but also allows for cluster friendly ONE-TIME submission(instead of manual freeze/unfreeze) setup demonstrated later.\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.feature_dim = 1280  # MobileNetV2 feature output size\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, proj_dim),\n",
        "            nn.BatchNorm1d(proj_dim), #Batch Normalization. Projected dimension is 256, which is the planned size of the text embeddings that will come from the text-based unimodal learning.\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x) #1280 feature output\n",
        "        projected = self.projection(features) # linear+normalization+Relu\n",
        "        return projected# returns processed batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70434e05",
      "metadata": {
        "id": "70434e05"
      },
      "source": [
        "Image Classifier Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7cecbe7",
      "metadata": {
        "id": "c7cecbe7"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Linear(256, 64),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.2),\n",
        "          nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.encoder(x)\n",
        "        logits = self.classifier(embeddings)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0b8b14",
      "metadata": {
        "id": "2f0b8b14"
      },
      "source": [
        "Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fe6599",
      "metadata": {
        "id": "05fe6599"
      },
      "outputs": [],
      "source": [
        "encoder = ImageEncoder(pretrained=True, freeze_backbone=True, proj_dim=256)\n",
        "model = ImageClassifier(encoder).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3890d8",
      "metadata": {
        "id": "fb3890d8"
      },
      "source": [
        "Training Loop Definition: Few words on this part. Initial design was to manually freeze/unfreeze pretrained weights as demonstrated in class, however, due to considerations given to potential need to submit to TALC, such design became infeasible because the possibility of having to submit MULTIPLE jobs and having ANOTHER submission come in between our submissions and I'm unsure how it will affect the model's performance, or can the save point be even accessed correctly if a different request was processed in between. Decision: make the training/validation loop automatically time freeze/unfreeze and process the entire process in 1 submission. In between freeze/unfreeze the optimizer learning rate as well as # of transfer learning epochs will also be tuned to avoid overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62875df9",
      "metadata": {
        "id": "62875df9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device,\n",
        "                epochs=EPOCHS,               # initial # of epochs used for model initialization\n",
        "                unfreeze_epoch=UNFREEZE_EPOCH,  # epoch after which the backbone is unfrozen\n",
        "                initial_lr=INITIAL_LR,       # learning rate before unfreeze\n",
        "                fine_tune_lr=FINE_TUNE_LR,   # learning rate after unfreeze\n",
        "                save_path=SAVE_MODEL_PATH):  # file path to save best val-loss model, Saving to FILE may be required, the model will be used later in multi-modal training once combined with text portion.\n",
        "\n",
        "    # ---- Loss ----\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ---- Initial optimizer: AdamW ----\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=initial_lr,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # This is how the automatic unfreeze works, once it reaches the defined epoch, backbone unfreezes and AdamW's lr is changed to fine_tune_lr.\n",
        "        if epoch == unfreeze_epoch:\n",
        "            print(\"Unfreezing backbone...\")\n",
        "            for param in model.encoder.backbone.parameters():\n",
        "                param.requires_grad = True # Unfreeze happens\n",
        "\n",
        "            # Updating lr in AdamW\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=fine_tune_lr,\n",
        "                weight_decay=1e-4\n",
        "            )\n",
        "\n",
        "        #Training\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad() #resets gradient between batches\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step() #Steps only after back-propagation\n",
        "            running_train_loss += loss.item()\n",
        "        epoch_train_loss = running_train_loss / len(train_loader) #training loss\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        #Validation\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad(): #thought this was redundant with model.eval(), but after learning, apparently not.\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "        epoch_val_loss = running_val_loss / len(val_loader) #validation loss\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} \" #Prints losses\n",
        "              f\"Train Loss: {epoch_train_loss:.4f} \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        # Saving model with smallest val_loss\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"Saved best model.\")\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd18a7e",
      "metadata": {
        "id": "6dd18a7e"
      },
      "source": [
        "Full-filling requirements to track misclassified images(Unimodal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a5b994",
      "metadata": {
        "id": "53a5b994"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_log_errors(model, test_loader, device,\n",
        "                            save_model_path=SAVE_MODEL_PATH,\n",
        "                            error_log_file=MISCLASS_FILE):\n",
        "\n",
        "    model.load_state_dict(torch.load(save_model_path))\n",
        "    model.eval()\n",
        "\n",
        "    incorrect_samples = []\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0) #Automatic resizing\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Log misclassified images\n",
        "            for i in range(len(labels)):\n",
        "                if preds[i] != labels[i]:\n",
        "                    dataset_index = batch_idx * test_loader.batch_size + i\n",
        "                    path, _ = test_loader.dataset.samples[dataset_index]\n",
        "                    incorrect_samples.append(\n",
        "                        f\"{path} | Pred: {preds[i].item()} | True: {labels[i].item()}\"\n",
        "                    )\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    with open(error_log_file, \"w\") as f:\n",
        "        for line in incorrect_samples:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Misclassified samples written to {error_log_file}\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8047a209",
      "metadata": {
        "id": "8047a209"
      },
      "source": [
        "Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e7d7ec",
      "metadata": {
        "id": "f3e7d7ec"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses, save_path=LOSS_PLOT_PATH):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"Loss curve saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c0760a",
      "metadata": {
        "id": "a1c0760a"
      },
      "source": [
        "Running model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a372aab6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a372aab6",
        "outputId": "ad07f7b7-5120-46d2-e963-318457e5dd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 Train Loss: 1.2863 Val Loss: 1.2244\n",
            "Saved best model.\n",
            "Epoch 2/30 Train Loss: 0.9867 Val Loss: 1.0479\n",
            "Saved best model.\n",
            "Epoch 3/30 Train Loss: 0.7630 Val Loss: 1.0555\n",
            "Epoch 4/30 Train Loss: 0.6719 Val Loss: 1.0646\n",
            "Epoch 5/30 Train Loss: 0.5884 Val Loss: 1.1415\n",
            "Epoch 6/30 Train Loss: 0.5492 Val Loss: 1.1402\n",
            "Epoch 7/30 Train Loss: 0.4881 Val Loss: 1.2106\n",
            "Epoch 8/30 Train Loss: 0.5145 Val Loss: 1.2493\n",
            "Epoch 9/30 Train Loss: 0.3640 Val Loss: 1.3435\n",
            "Epoch 10/30 Train Loss: 0.3563 Val Loss: 1.3346\n",
            "Epoch 11/30 Train Loss: 0.2795 Val Loss: 1.3579\n",
            "Epoch 12/30 Train Loss: 0.3252 Val Loss: 1.3922\n",
            "Epoch 13/30 Train Loss: 0.3872 Val Loss: 1.3999\n",
            "Epoch 14/30 Train Loss: 0.2876 Val Loss: 1.2275\n",
            "Epoch 15/30 Train Loss: 0.3588 Val Loss: 1.3622\n",
            "Epoch 16/30 Train Loss: 0.2847 Val Loss: 1.3425\n",
            "Epoch 17/30 Train Loss: 0.3078 Val Loss: 1.5503\n",
            "Epoch 18/30 Train Loss: 0.3032 Val Loss: 1.4024\n",
            "Epoch 19/30 Train Loss: 0.1978 Val Loss: 1.4638\n",
            "Epoch 20/30 Train Loss: 0.2145 Val Loss: 1.5913\n",
            "Epoch 21/30 Train Loss: 0.3013 Val Loss: 1.6701\n",
            "Epoch 22/30 Train Loss: 0.2634 Val Loss: 1.5280\n",
            "Epoch 23/30 Train Loss: 0.2659 Val Loss: 1.6338\n",
            "Epoch 24/30 Train Loss: 0.1762 Val Loss: 1.5124\n",
            "Unfreezing backbone...\n",
            "Epoch 25/30 Train Loss: 0.2565 Val Loss: 1.4105\n",
            "Epoch 26/30 Train Loss: 0.1731 Val Loss: 1.4992\n",
            "Epoch 27/30 Train Loss: 0.2214 Val Loss: 1.5426\n",
            "Epoch 28/30 Train Loss: 0.2438 Val Loss: 1.4918\n",
            "Epoch 29/30 Train Loss: 0.1405 Val Loss: 1.5412\n",
            "Epoch 30/30 Train Loss: 0.2294 Val Loss: 1.4343\n",
            "Loss curve saved to loss_curve.png\n",
            "Test Accuracy: 0.4410\n",
            "Misclassified samples written to misclassified.txt\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses = train_model(model, train_loader, val_loader, device)\n",
        "plot_losses(train_losses, val_losses)\n",
        "accuracy = evaluate_and_log_errors(model, test_loader, device)\n",
        "imagemodel = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceOr8M1JLEGG",
      "metadata": {
        "id": "ceOr8M1JLEGG"
      },
      "source": [
        "Retrieve Image Embeddings for Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "vDu8okH1vcyO",
      "metadata": {
        "id": "vDu8okH1vcyO"
      },
      "outputs": [],
      "source": [
        "def get_image_embeddings(images, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # MobileNetV2 has a .features attribute\n",
        "        # We pass it through features, then pool it to get a 1D vector\n",
        "        x = model.features(images)\n",
        "        x = nn.AdaptiveAvgPool2d(1)(x)\n",
        "        embeddings = torch.flatten(x, 1)\n",
        "    return embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tHJ1FudPLnxo",
      "metadata": {
        "id": "tHJ1FudPLnxo"
      },
      "source": [
        "Text-based DistillBert Classification:\n",
        "Uses distilbert-base-uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cRbpVDeVLyQh",
      "metadata": {
        "id": "cRbpVDeVLyQh"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "CP3X9zSQLzYx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP3X9zSQLzYx",
        "outputId": "dfa6469e-05f9-4264-a026-dafb02851225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, DistilBertForSequenceClassification\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_4VtQwEFMSLe",
      "metadata": {
        "id": "_4VtQwEFMSLe"
      },
      "source": [
        "Tokenizer+Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "e-qCm_iwL4Ln",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613,
          "referenced_widgets": [
            "633417402edc43acbfd53d6b6f427eb3",
            "48b8283a8d764a329c0d2a1f864b4c79",
            "f19bb509083b416b9704b2559c818131",
            "c6f857605299401fb919d048f6acb513",
            "48f15d03676e482f926c489d83502ca8",
            "4eb6275b7ed448118b9eea27d4f2d0ac",
            "ab6f0e3b378a4da4ac3bc1f3c9120af0",
            "08a5650ee1ac47a98deea0f240c88016",
            "cf4dff4016474fbdbedfa693bc2eca36",
            "6be7fc492ce540cab78c218c8881901c",
            "a9370791d8104420b3a58c11df058a1c"
          ]
        },
        "id": "e-qCm_iwL4Ln",
        "outputId": "9bf5864c-abac-4838-d30e-0da32875580d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17c831f89e7f41fda14633c3dc383d3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "276673c4847346a996f359c21c5fff77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19149ec1531d452e8aea720d77338f81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34de9068576d4de4b9bf4a021b8bdd34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4460e13dd79f4029b4c0ca90fc520761",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "062320e6d54b42939d740abb2e9bc34e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "vocab_transform.bias    | UNEXPECTED |  | \n",
            "vocab_projector.bias    | UNEXPECTED |  | \n",
            "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
            "vocab_transform.weight  | UNEXPECTED |  | \n",
            "vocab_layer_norm.weight | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "base_model = base_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WGSjkJufOOZo",
      "metadata": {
        "id": "WGSjkJufOOZo"
      },
      "source": [
        "Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "o1axYc69OP0b",
      "metadata": {
        "id": "o1axYc69OP0b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def read_text_files_with_labels(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    class_folders = sorted([\n",
        "    f for f in os.listdir(path)\n",
        "    if os.path.isdir(os.path.join(path, f)) and not f.startswith(\".\")\n",
        "]) #Avoids hidden files being picked up\n",
        "\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(path, class_name)\n",
        "\n",
        "        if os.path.isdir(class_path):\n",
        "            file_names = os.listdir(class_path)\n",
        "\n",
        "            for file_name in file_names:\n",
        "                file_path = os.path.join(class_path, file_name)\n",
        "\n",
        "                if os.path.isfile(file_path):\n",
        "                    # Remove extension\n",
        "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "\n",
        "                    # Replace \"_\" with space\n",
        "                    text = file_name_no_ext.replace('_', ' ')\n",
        "\n",
        "                    # Remove digits\n",
        "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
        "\n",
        "                    texts.append(text_without_digits)\n",
        "                    labels.append(label_map[class_name])\n",
        "\n",
        "    return np.array(texts), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjj2hu4-OTMg",
      "metadata": {
        "id": "wjj2hu4-OTMg"
      },
      "source": [
        "Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1GQsdnzyOV2W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GQsdnzyOV2W",
        "outputId": "83add0bc-87c4-4b05-d7b4-5c462cbbfabe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 200\n",
            "Val samples: 205\n",
            "Test samples: 161\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = \"dataset\"\n",
        "\n",
        "train_texts, train_labels = read_text_files_with_labels(os.path.join(DATA_ROOT, \"train\"))\n",
        "val_texts, val_labels = read_text_files_with_labels(os.path.join(DATA_ROOT, \"val\"))\n",
        "test_texts, test_labels = read_text_files_with_labels(os.path.join(DATA_ROOT, \"test\"))\n",
        "\n",
        "print(\"Train samples:\", len(train_texts))\n",
        "print(\"Val samples:\", len(val_texts))\n",
        "print(\"Test samples:\", len(test_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8ba5d994",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected classes and their labels: {'Black': 0, 'Blue': 1, 'Green': 2, 'Other': 3}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the root dataset folder\n",
        "dataset_root = \"dataset\"\n",
        "\n",
        "# List of subfolders to scan\n",
        "subfolders = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "# Collect all class folders across all subfolders\n",
        "all_class_folders = set()\n",
        "\n",
        "for subfolder in subfolders:\n",
        "    path = os.path.join(dataset_root, subfolder)\n",
        "    # Only include directories that don't start with a dot\n",
        "    class_folders = [\n",
        "        f for f in os.listdir(path)\n",
        "        if os.path.isdir(os.path.join(path, f)) and not f.startswith(\".\")\n",
        "    ]\n",
        "    all_class_folders.update(class_folders)\n",
        "\n",
        "# Sort class names and create label map\n",
        "all_class_folders = sorted(all_class_folders)\n",
        "label_map = {class_name: idx for idx, class_name in enumerate(all_class_folders)}\n",
        "\n",
        "print(\"Detected classes and their labels:\", label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e175577b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.DS_Store', 'Black', 'Blue', 'Green', 'Other']\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"dataset/test\"  # adjust if needed\n",
        "print(sorted(os.listdir(dataset_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-gD6LR-rL7Th",
      "metadata": {
        "id": "-gD6LR-rL7Th"
      },
      "source": [
        "Text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vvgl-phHL9Tu",
      "metadata": {
        "id": "vvgl-phHL9Tu"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 500\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ru_lJZ_VMHbw",
      "metadata": {
        "id": "Ru_lJZ_VMHbw"
      },
      "source": [
        "Data-Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "1kHDSY0TMJxa",
      "metadata": {
        "id": "1kHDSY0TMJxa"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 16 #Roughly 50 per class in this setup\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset   = TextDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset  = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o12l0oMeMYpa",
      "metadata": {
        "id": "o12l0oMeMYpa"
      },
      "source": [
        "Classification Head(with dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "Nra0seb3Mgsw",
      "metadata": {
        "id": "Nra0seb3Mgsw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(384, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use CLS token representation\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [B, 384]\n",
        "\n",
        "        x = self.dropout(cls_embedding)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J8FJoTYDMjm0",
      "metadata": {
        "id": "J8FJoTYDMjm0"
      },
      "source": [
        "Model Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "JklBeOIqMncu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "JklBeOIqMncu",
        "outputId": "4abd197e-6577-4f25-a769-667906c037cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected classes: 4\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(np.unique(train_labels))\n",
        "print(\"Detected classes:\", num_classes)\n",
        "\n",
        "model = TextClassifier(base_model, num_classes=num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O9EbfTMKMp4n",
      "metadata": {
        "id": "O9EbfTMKMp4n"
      },
      "source": [
        "Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "Nu94I0EKMtAx",
      "metadata": {
        "id": "Nu94I0EKMtAx"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sarlSGzqMug3",
      "metadata": {
        "id": "sarlSGzqMug3"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "wBWw_fukMwky",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "wBWw_fukMwky",
        "outputId": "9624fd5f-f32d-4d72-bb3c-9efc92d3bdb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "Train Loss: 0.1107\n",
            "Train Acc:  0.9800\n",
            "Val Loss:   1.2654\n",
            "Val Acc:    0.6976\n",
            "----------------------------------------\n",
            "Epoch 2/15\n",
            "Train Loss: 0.0965\n",
            "Train Acc:  0.9900\n",
            "Val Loss:   1.2804\n",
            "Val Acc:    0.6732\n",
            "----------------------------------------\n",
            "Epoch 3/15\n",
            "Train Loss: 0.0990\n",
            "Train Acc:  0.9800\n",
            "Val Loss:   1.3111\n",
            "Val Acc:    0.6780\n",
            "----------------------------------------\n",
            "Epoch 4/15\n",
            "Train Loss: 0.1176\n",
            "Train Acc:  0.9700\n",
            "Val Loss:   1.2929\n",
            "Val Acc:    0.6732\n",
            "----------------------------------------\n",
            "Epoch 5/15\n",
            "Train Loss: 0.0959\n",
            "Train Acc:  0.9750\n",
            "Val Loss:   1.2862\n",
            "Val Acc:    0.6878\n",
            "----------------------------------------\n",
            "Epoch 6/15\n",
            "Train Loss: 0.0930\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3028\n",
            "Val Acc:    0.6927\n",
            "----------------------------------------\n",
            "Epoch 7/15\n",
            "Train Loss: 0.0926\n",
            "Train Acc:  0.9900\n",
            "Val Loss:   1.4375\n",
            "Val Acc:    0.6146\n",
            "----------------------------------------\n",
            "Epoch 8/15\n",
            "Train Loss: 0.0968\n",
            "Train Acc:  0.9800\n",
            "Val Loss:   1.3579\n",
            "Val Acc:    0.6537\n",
            "----------------------------------------\n",
            "Epoch 9/15\n",
            "Train Loss: 0.0909\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3994\n",
            "Val Acc:    0.6488\n",
            "----------------------------------------\n",
            "Epoch 10/15\n",
            "Train Loss: 0.0864\n",
            "Train Acc:  0.9900\n",
            "Val Loss:   1.3160\n",
            "Val Acc:    0.6585\n",
            "----------------------------------------\n",
            "Epoch 11/15\n",
            "Train Loss: 0.0895\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3060\n",
            "Val Acc:    0.6878\n",
            "----------------------------------------\n",
            "Epoch 12/15\n",
            "Train Loss: 0.0896\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.2936\n",
            "Val Acc:    0.6780\n",
            "----------------------------------------\n",
            "Epoch 13/15\n",
            "Train Loss: 0.0783\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3114\n",
            "Val Acc:    0.6927\n",
            "----------------------------------------\n",
            "Epoch 14/15\n",
            "Train Loss: 0.0824\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3246\n",
            "Val Acc:    0.6780\n",
            "----------------------------------------\n",
            "Epoch 15/15\n",
            "Train Loss: 0.0700\n",
            "Train Acc:  0.9850\n",
            "Val Loss:   1.3248\n",
            "Val Acc:    0.6683\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "misclassified_files = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    model.eval()\n",
        "    val_loss_total = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            file_names = batch.get(\"file_name\", [None]*labels.size(0))  # fallback if not present\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collecting misclassified file names\n",
        "            for i in range(len(labels)):\n",
        "                if preds[i] != labels[i] and file_names[i] is not None:\n",
        "                    misclassified_files.append(file_names[i])\n",
        "\n",
        "    val_loss = val_loss_total / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Logging\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Train Acc:  {train_acc:.4f}\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
        "    print(f\"Val Acc:    {val_acc:.4f}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d87e8b",
      "metadata": {},
      "source": [
        "Saving list of misclassified images to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "3fc966f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"misclassified.txt\", \"w\") as f:\n",
        "    for file_name in misclassified_files:\n",
        "        f.write(f\"{file_name}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aeaa3e3",
      "metadata": {},
      "source": [
        "Loss Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "6d91f7b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, EPOCHS+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, EPOCHS+1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7mJqDfluM5Wp",
      "metadata": {
        "id": "7mJqDfluM5Wp"
      },
      "source": [
        "Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "cT8qnsGhM7uC",
      "metadata": {
        "id": "cT8qnsGhM7uC"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"clean_text_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "mklsViI81f9c",
      "metadata": {
        "id": "mklsViI81f9c"
      },
      "outputs": [],
      "source": [
        "def extract_final_text_embeddings(text_list, model_path, tokenizer, base_model):\n",
        "    \"\"\"\n",
        "    text_list : list of strings\n",
        "    model_path: path to your trained classifier\n",
        "    tokenizer  : the tokenizer used for DistilBERT\n",
        "    base_model : the underlying DistilBERT model (without the classifier head)\n",
        "    \"\"\"\n",
        "    # Load the trained classifier\n",
        "    trained_model = TextClassifier(num_classes=4, base_model=base_model).to(device)\n",
        "    trained_model.load_state_dict(torch.load(model_path))\n",
        "    trained_model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in text_list:\n",
        "            # Tokenize\n",
        "            encoding = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            ).to(device)\n",
        "\n",
        "            # Pass through the base model only\n",
        "            outputs = trained_model.base_model(\n",
        "                input_ids=encoding[\"input_ids\"],\n",
        "                attention_mask=encoding[\"attention_mask\"]\n",
        "            )\n",
        "\n",
        "            # DistilBERT returns last_hidden_state (batch, seq_len, hidden_size)\n",
        "            # Use [CLS]-like token embedding (first token) or mean pooling\n",
        "            token_embeddings = outputs.last_hidden_state  # (1, seq_len, hidden_size)\n",
        "            attention_mask = encoding[\"attention_mask\"]\n",
        "\n",
        "            # Mean pooling\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "            sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "            embedding = sum_embeddings / sum_mask  # (1, hidden_size)\n",
        "\n",
        "            all_embeddings.append(embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "    return np.array(all_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Dvihtf82NVM",
      "metadata": {
        "id": "6Dvihtf82NVM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF_ENV",
      "language": "python",
      "name": "tf_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08a5650ee1ac47a98deea0f240c88016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b8283a8d764a329c0d2a1f864b4c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb6275b7ed448118b9eea27d4f2d0ac",
            "placeholder": "​",
            "style": "IPY_MODEL_ab6f0e3b378a4da4ac3bc1f3c9120af0",
            "value": "Loading weights: 100%"
          }
        },
        "48f15d03676e482f926c489d83502ca8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb6275b7ed448118b9eea27d4f2d0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633417402edc43acbfd53d6b6f427eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48b8283a8d764a329c0d2a1f864b4c79",
              "IPY_MODEL_f19bb509083b416b9704b2559c818131",
              "IPY_MODEL_c6f857605299401fb919d048f6acb513"
            ],
            "layout": "IPY_MODEL_48f15d03676e482f926c489d83502ca8"
          }
        },
        "6be7fc492ce540cab78c218c8881901c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9370791d8104420b3a58c11df058a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab6f0e3b378a4da4ac3bc1f3c9120af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6f857605299401fb919d048f6acb513": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be7fc492ce540cab78c218c8881901c",
            "placeholder": "​",
            "style": "IPY_MODEL_a9370791d8104420b3a58c11df058a1c",
            "value": " 103/103 [00:00&lt;00:00, 404.05it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "cf4dff4016474fbdbedfa693bc2eca36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f19bb509083b416b9704b2559c818131": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a5650ee1ac47a98deea0f240c88016",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf4dff4016474fbdbedfa693bc2eca36",
            "value": 103
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
